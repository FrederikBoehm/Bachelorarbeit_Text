\chapter{Summary}\label{ch:summary}

This thesis contributed to various research areas.
During the research \ac{BERT} was sucessfully trained on Form 10-K's and Form 10-Q's by companys listed in the \ac{SP} 500 index, which were loaded from the \ac{SRAF}.
The resulting model beats the original BERT\textsubscript{BASE} model by 22.1\% in the masked language model task and by 34.1\% in the next sentence prediction and achieves similar results as FinBERT by \cite{DeSola2019}.
The thesis also showed how the \ac{BERT} sequence embeddings can be used to represent a whole document by averaging them per business report.
This allowed to train multiple classifiers, including a Naive Bayes, a \ac{KNN} classifier and a \ac{SVM}, reaching classification accuracies of up to 69.3\%.
A test of the hypothesis that the classifiers learned to distinguish a good from a bad company was negative.
Therefore it can be concluded that the models can classify the business reports and machine learning is indeed viable to predict short-term market reactions under the restrictions made in chapter \ref{ch:outlook}.
